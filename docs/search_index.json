[["index.html", "Collected R Code Introduction", " Collected R Code Phil Swatton Introduction This is a collection of R code written for the purposes of practice and demonstration. At the moment this is all in a work in progress stage. The long-term goal is to include lots of statistical and machine learning algorithms, covering model estimation and data reduction. I may also include some other interesting algorithms down the line. Beyond giving me both a venue for solidifying my understanding and acting as a sort of online advert for my skillset, I also hope that this document will end up being of use to some other people. "],["ordinary-least-squares.html", "Ordinary Least Squares", " Ordinary Least Squares ols &lt;- function(data, formula, method=&quot;QR&quot;, na.action=na.exclude) { mf &lt;- model.frame(formula, data, na.action=na.action) Y &lt;- model.response(mf, type=&quot;double&quot;) X &lt;- model.matrix(formula, mf) if (method == &quot;matrix&quot;) B &lt;- solve(crossprod(X)) %*% crossprod(X, Y) if (method == &quot;QR&quot;) { QR &lt;- qr(X) Q &lt;- qr.Q(QR) R &lt;- qr.R(QR) B &lt;- backsolve(R, t(Q)%*%Y) } return(B) } Testing the function: set.seed(42) N &lt;- 1000 X1 &lt;- rnorm(N) X2 &lt;- rnorm(N) y = 1.5 + 2*X1 + 3*X2 + rnorm(N) df &lt;- data.frame( y, X1, X2 ) f &lt;- y ~ X1 + X2 ols(df, f) ## [,1] ## [1,] 1.496133 ## [2,] 1.978201 ## [3,] 2.981712 ols(df, f, &quot;matrix&quot;) ## [,1] ## (Intercept) 1.496133 ## X1 1.978201 ## X2 2.981712 summary(lm(f, df)) ## ## Call: ## lm(formula = f, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1968 -0.6336 0.0163 0.6654 3.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.49613 0.03259 45.91 &lt;2e-16 *** ## X1 1.97820 0.03251 60.84 &lt;2e-16 *** ## X2 2.98171 0.03306 90.20 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.03 on 997 degrees of freedom ## Multiple R-squared: 0.923, Adjusted R-squared: 0.9228 ## F-statistic: 5974 on 2 and 997 DF, p-value: &lt; 2.2e-16 "],["logit.html", "Logit", " Logit inv_logit &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) return(p) } logit &lt;- function(data, formula, na.action=na.exclude) { mf &lt;- model.frame(formula, data, na.action=na.action) Y &lt;- model.response(mf, type=&quot;double&quot;) X &lt;- model.matrix(formula, mf) B &lt;- numeric(ncol(X)) logitll &lt;- function(B) { p &lt;- inv_logit((X %*% B)) lli &lt;- Y*log(p) + (1-Y)*log(1-p) return(-sum(lli)) } est &lt;- optim(B, logitll, method=&quot;BFGS&quot;) return(est$par) } Testing the function: set.seed(42) N &lt;- 1000 X1 &lt;- rnorm(N) X2 &lt;- rnorm(N) ystar &lt;- 1.5 + 2*X1 + 3*X2 + rnorm(N) y &lt;- rbinom(N, 1, inv_logit(ystar)) df &lt;- data.frame( y, X1, X2 ) f &lt;- y ~ X1 + X2 summary(glm(f,binomial,df))$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.413335 0.1203875 11.73988 7.960399e-32 ## X1 1.839204 0.1439189 12.77945 2.135770e-37 ## X2 2.472301 0.1728920 14.29968 2.198073e-46 logit(df, f) ## [1] 1.413368 1.839245 2.472373 "],["ordered-logit.html", "Ordered Logit", " Ordered Logit inv_logit &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) return(p) } ologit &lt;- function(data, formula, na.action=na.exclude) { mf &lt;- model.frame(formula, data, na.action=na.action) Y &lt;- model.response(mf) X &lt;- model.matrix(formula, mf)[,-1] N &lt;- nrow(X) levels &lt;- unique(Y) |&gt; sort() M &lt;- length(levels) ncut &lt;- M-1 c &lt;- 1:ncut/M b &lt;- numeric(ncol(X)) par &lt;- c(c,b) ologitLL &lt;- function(param) { c &lt;- param[1:ncut] b &lt;- param[(ncut+1):length(param)] Xb &lt;- X %*% b lli &lt;- numeric(N) for (m in 1:M) { if (m == 1) { lli[Y==levels[m]] &lt;- log(inv_logit(c[m]-Xb[Y==levels[m]])) } else if (m &lt; M) { lli[Y==levels[m]] &lt;- log(inv_logit(c[m]-Xb[Y==levels[m]]) - inv_logit(c[m-1]-Xb[Y==levels[m]])) } else { lli[Y==levels[m]] &lt;- log(1 - inv_logit(c[m-1]-Xb[Y==levels[m]])) } } return(-sum(lli)) } out &lt;- optim(par, ologitLL, method=&quot;BFGS&quot;) est &lt;- out$par names(est)[1:ncut] &lt;- paste0(&quot;t&quot;, 1:ncut) names(est)[(ncut+1):length(est)] &lt;- colnames(X) return(est) } Testing the function: # Example from https://towardsdatascience.com/implementing-and-interpreting-ordinal-logistic-regression-1ee699274cf5 library(carData) library(MASS) data(WVS) f &lt;- poverty~religion+degree+country+age+gender summary(polr(f, WVS)) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = f, data = WVS) ## ## Coefficients: ## Value Std. Error t value ## religionyes 0.17973 0.077346 2.324 ## degreeyes 0.14092 0.066193 2.129 ## countryNorway -0.32235 0.073766 -4.370 ## countrySweden -0.60330 0.079494 -7.589 ## countryUSA 0.61777 0.070665 8.742 ## age 0.01114 0.001561 7.139 ## gendermale 0.17637 0.052972 3.329 ## ## Intercepts: ## Value Std. Error t value ## Too Little|About Right 0.7298 0.1041 7.0128 ## About Right|Too Much 2.5325 0.1103 22.9496 ## ## Residual Deviance: 10402.59 ## AIC: 10420.59 ologit(WVS, f) ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## t1 t2 religionyes degreeyes countryNorway ## 0.72937251 2.53210358 0.17978268 0.14087551 -0.32235825 ## countrySweden countryUSA age gendermale ## -0.60330084 0.61782030 0.01113128 0.17637937 "],["multinomial-logit.html", "Multinomial Logit", " Multinomial Logit mnlogit &lt;- function(data, formula, na.action=na.exclude) { mf &lt;- model.frame(formula, data, na.action=na.action) y &lt;- model.response(mf) X &lt;- model.matrix(formula, mf) N &lt;- nrow(X) K &lt;- ncol(X) levels &lt;- unique(y) |&gt; sort() M &lt;- length(levels) Y &lt;- matrix(N*M, nrow=N, ncol=M) for (m in 1:M) { Y[,m] &lt;- ifelse(y==levels[m], 1, 0) } b &lt;- numeric(K*(M-1)) mnlogitLL &lt;- function(param) { b &lt;- matrix(param, nrow=K, ncol=M-1) Xb &lt;- cbind(rep(0, N), X %*% b) lli &lt;- numeric(N) for (m in 1:M) { lli &lt;- lli + Y[,m]*Xb[,m] - Y[,m]*log(rowSums(exp(Xb))) } return(-sum(lli)) } out &lt;- optim(b, mnlogitLL, method=&quot;BFGS&quot;) est &lt;- matrix(out$par, nrow=K, ncol=M-1) rownames(est) &lt;- colnames(X) colnames(est) &lt;- paste0(levels[1], &quot;/&quot;, levels[2:M]) return(est) } Testing the function: library(mclogit) ## Loading required package: Matrix housing &lt;- MASS::housing #has an ordinal outcome but we&#39;ll ignore that for our purposes f &lt;- Sat ~ Infl + Freq + Type t(mblogit(f, data = housing)$coefmat) ## ## Iteration 1 - deviance = 150.7444 - criterion = 0.5116897 ## Iteration 2 - deviance = 150.511 - criterion = 0.001549817 ## Iteration 3 - deviance = 150.5107 - criterion = 1.965799e-06 ## Iteration 4 - deviance = 150.5107 - criterion = 3.804011e-12 ## converged ## Response categories ## Predictors Medium High ## (Intercept) 1.1352190 -0.740346318 ## InflMedium 0.1291512 0.005033149 ## InflHigh -0.4256812 0.282489556 ## Freq -0.0501481 0.026990219 ## TypeApartment 0.8092387 -0.644372087 ## TypeAtrium -0.4053420 0.264568900 ## TypeTerrace -0.3689781 0.189799430 mnlogit(housing, f) ## Low/Medium Low/High ## (Intercept) 1.13582927 -0.740243313 ## InflMedium 0.12920635 0.005028019 ## InflHigh -0.42591879 0.282447109 ## Freq -0.05017388 0.026985033 ## TypeApartment 0.80963960 -0.644298089 ## TypeAtrium -0.40569533 0.264391915 ## TypeTerrace -0.36904115 0.189905490 "],["principal-component-analysis.html", "Principal Component Analysis", " Principal Component Analysis pca &lt;- function(data, scale=T, center=T) { data &lt;- data[complete.cases(data),] if (center) mat &lt;- apply(data, 2, function(x) x - mean(x)) else mat &lt;- data if (scale) for(i in 1:ncol(mat)) mat[,i] &lt;- mat[,i] / sd(data[,i]) decomp &lt;- svd(mat) var &lt;- (decomp$d^2)/(nrow(mat)-1) loadings &lt;- decomp$v colnames(loadings) &lt;- paste0(&quot;PC&quot;, 1:ncol(loadings)) rownames(loadings) &lt;- colnames(mat) return(list(var=var, loadings=loadings)) } Testing the function: # Dataset of English constituency vote shares from UK GE2019 library(parlitools) df &lt;- subset(bes_2019, subset=bes_2019$country==&quot;England&quot;, select=c(&quot;con_19&quot;, &quot;lab_19&quot;, &quot;ld_19&quot;, &quot;brexit_19&quot;, &quot;green_19&quot;, &quot;other_19&quot;)) df &lt;- apply(df, 2, function(x) ifelse(is.na(x), 0, x)) # With centering and rescaling pc &lt;- prcomp(df, scale.=T) pc$sdev^2 ## [1] 2.372841e+00 1.121676e+00 1.022484e+00 8.585543e-01 6.244452e-01 ## [6] 3.622418e-31 pc$rotation ## PC1 PC2 PC3 PC4 PC5 PC6 ## con_19 -0.54332983 0.07627559 -0.05082277 -0.56942560 0.1384571 0.5941178 ## lab_19 0.59823413 -0.07011487 -0.19959561 0.02259843 -0.4083210 0.6558389 ## ld_19 -0.39181511 -0.36512555 0.04767122 0.74424233 0.1153703 0.3790577 ## brexit_19 0.43609277 -0.25251232 0.24198884 -0.11374592 0.8065703 0.1549459 ## green_19 0.04764488 0.75339583 -0.43749954 0.30622933 0.3617630 0.1186169 ## other_19 0.03070171 0.47390033 0.83984620 0.12096531 -0.1391588 0.1874472 pca(df) ## $var ## [1] 2.372841e+00 1.121676e+00 1.022484e+00 8.585543e-01 6.244452e-01 ## [6] 3.628914e-31 ## ## $loadings ## PC1 PC2 PC3 PC4 PC5 PC6 ## con_19 -0.54332983 0.07627559 -0.05082277 -0.56942560 0.1384571 -0.5941178 ## lab_19 0.59823413 -0.07011487 -0.19959561 0.02259843 -0.4083210 -0.6558389 ## ld_19 -0.39181511 -0.36512555 0.04767122 0.74424233 0.1153703 -0.3790577 ## brexit_19 0.43609277 -0.25251232 0.24198884 -0.11374592 0.8065703 -0.1549459 ## green_19 0.04764488 0.75339583 -0.43749954 0.30622933 0.3617630 -0.1186169 ## other_19 0.03070171 0.47390033 0.83984620 0.12096531 -0.1391588 -0.1874472 # Without centering and rescaling pc &lt;- prcomp(df, center=F, scale.=F) pc$sdev^2 ## [1] 3601.582679 528.759516 102.026004 26.105736 14.712369 9.887987 pc$rotation ## PC1 PC2 PC3 PC4 PC5 ## con_19 -0.79601894 0.54348891 0.26225689 -0.02744832 0.02841558 ## lab_19 -0.57199009 -0.81268134 -0.05235555 -0.01872792 -0.07167662 ## ld_19 -0.18775368 0.19101255 -0.96262869 0.03485081 0.01576254 ## brexit_19 -0.03580223 -0.08665429 0.01318959 0.10999656 0.93237245 ## green_19 -0.04785633 -0.01272207 0.01544479 0.07000400 -0.34354616 ## other_19 -0.01891353 0.00349730 0.03759933 0.99029363 -0.08040029 ## PC6 ## con_19 -0.02518186 ## lab_19 -0.06439723 ## ld_19 0.01207155 ## brexit_19 0.33108554 ## green_19 0.93508548 ## other_19 -0.10521720 pca(df, center=F, scale=F) ## $var ## [1] 3601.582679 528.759516 102.026004 26.105736 14.712369 9.887987 ## ## $loadings ## PC1 PC2 PC3 PC4 PC5 ## con_19 -0.79601894 0.54348891 0.26225689 -0.02744832 0.02841558 ## lab_19 -0.57199009 -0.81268134 -0.05235555 -0.01872792 -0.07167662 ## ld_19 -0.18775368 0.19101255 -0.96262869 0.03485081 0.01576254 ## brexit_19 -0.03580223 -0.08665429 0.01318959 0.10999656 0.93237245 ## green_19 -0.04785633 -0.01272207 0.01544479 0.07000400 -0.34354616 ## other_19 -0.01891353 0.00349730 0.03759933 0.99029363 -0.08040029 ## PC6 ## con_19 -0.02518186 ## lab_19 -0.06439723 ## ld_19 0.01207155 ## brexit_19 0.33108554 ## green_19 0.93508548 ## other_19 -0.10521720 "],["lasso-regression---l1-regularization.html", "LASSO Regression - L1 Regularization", " LASSO Regression - L1 Regularization lasso &lt;- function(X, Y, rm_na=T, standardise=F, lambda, tol=1e-6, max_iter=1e+5) { if (!is.matrix(X)) { X &lt;- as.matrix(X) } if (rm_na) { index &lt;- complete.cases(X) &amp; complete.cases(Y) X &lt;- X[index,] Y &lt;- Y[index] } if (standardise) { X &lt;- apply(X, 2, function(x) (x - mean(x))/sd(x)) } # if (intercept) { # X &lt;- cbind(rep(1,nrow(X)), X) # colnames(X)[1] &lt;- &quot;(Intercept)&quot; # } # optim&#39;s methods won&#39;t produce variable selection - see https://stats.stackexchange.com/questions/121209/how-can-i-implement-lasso-in-r-using-optim-function # so estimate via coordinate descent K &lt;- ncol(X) b &lt;- numeric(ncol(X)) # b &lt;- solve(crossprod(X) + lambda*diag(K)) %*% crossprod(X, Y) names(b) &lt;- colnames(X) soft_thresh &lt;- function(b, l) { ifelse(l &lt; abs(b), sign(b)*(abs(b) - l), 0) } current &lt;- 1 for (iter in 1:max_iter) { b_old &lt;- b for (k in 1:K) { r &lt;- Y - X[,-k] %*% b[-k] b[k] &lt;- soft_thresh(crossprod(X[,k],r), length(Y)*lambda)/crossprod(X[,k]) #length(y) gives consistent results w/ glmnet } current &lt;- norm(as.matrix(b-b_old), &quot;F&quot;) if (which.min(c(tol,current))==2) break if (any(is.na(b)) | any(is.nan(b))) break } return(b) } Testing the function: library(glmnet) ## Loaded glmnet 4.1-4 X &lt;- as.matrix(mtcars[, -1]) X_standard &lt;- apply(X, 2, function(x) (x-mean(x)) / sd(x)) Y &lt;- mtcars[[1]] data.frame( glmnet = glmnet(X, Y, alpha=1, lambda=0.5, thresh = 1e-16, intercept = F, standardize=F)$beta[,1], lasso = lasso(X, Y, lambda=0.5) ) ## glmnet lasso ## cyl 0.000000000 0.000000000 ## disp -0.012341996 -0.012342881 ## hp -0.007107688 -0.007107273 ## drat 1.226803494 1.226801727 ## wt -1.034688395 -1.034551274 ## qsec 0.993392862 0.993371825 ## vs 0.000000000 0.000000000 ## am 0.000000000 0.000000000 ## gear 1.637349031 1.637404538 ## carb -0.340430706 -0.340473978 data.frame( glmnet = glmnet(X_standard, Y, alpha=1, lambda=0.5, thresh = 1e-16, intercept = F, standardize=F)$beta[,1], lasso = lasso(X_standard, Y, lambda=0.5) ) ## glmnet lasso ## cyl -1.53700736 -1.53700527 ## disp 0.00000000 0.00000000 ## hp -0.96091454 -0.96091464 ## drat 0.03332515 0.03332529 ## wt -2.62683276 -2.62683457 ## qsec 0.00000000 0.00000000 ## vs 0.00000000 0.00000000 ## am 0.22850288 0.22850260 ## gear 0.00000000 0.00000000 ## carb -0.16064907 -0.16064929 "],["ridge-regression---l2-regularization.html", "Ridge Regression - L2 Regularization", " Ridge Regression - L2 Regularization ridge &lt;- function(X, Y, rm_na=T, standardise=F, lambda, method=&quot;matrix&quot;) { if (!is.matrix(X)) { X &lt;- as.matrix(X) } if (rm_na) { index &lt;- complete.cases(X) &amp; complete.cases(Y) X &lt;- X[index,] Y &lt;- Y[index] } if (standardise) { X &lt;- apply(X, 2, function(x) (x - mean(x))/sd(x)) } # if (intercept) { # X &lt;- cbind(rep(1,nrow(X)), X) # colnames(X)[1] &lt;- &quot;(Intercept)&quot; # } k &lt;- ncol(X) if (method==&quot;matrix&quot;) { b &lt;- solve(crossprod(X) + lambda*diag(k)) %*% crossprod(X, Y) } else if (method==&quot;MLE&quot;) { start &lt;- numeric(k) ridgeMin &lt;- function(b) { crossprod(Y - X%*%b) + lambda * crossprod(b) } out &lt;- optim(start, ridgeMin, method=&quot;BFGS&quot;) b &lt;- out$par names(b) &lt;- colnames(X) } return(b) } Testing the function: library(glmnet) X &lt;- as.matrix(mtcars[, -1]) X_standard &lt;- apply(X, 2, function(x) (x-mean(x)) / sd(x)) Y &lt;- mtcars[[1]] data.frame( glmnet = glmnet(X, Y, alpha=0, lambda=0.5, thresh = 1e-16, intercept = F, standardize=F)$beta[,1], ridge = ridge(X, Y, lambda=0.5) ) ## glmnet ridge ## cyl 0.374790786 0.369554729 ## disp 0.006183499 0.008205706 ## hp -0.016705862 -0.017783212 ## drat 1.435599127 1.392774438 ## wt -2.984416583 -3.210741706 ## qsec 1.048304867 1.083489203 ## vs 0.043144589 0.078258643 ## am 2.282659223 2.425849951 ## gear 1.376685079 1.299391499 ## carb -0.537955121 -0.469904311 data.frame( glmnet = glmnet(X_standard, Y, alpha=0, lambda=0.5, thresh = 1e-16, intercept = F, standardize=F)$beta[,1], ridge = ridge(X_standard, Y, lambda=0.5) ) ## glmnet ridge ## cyl -0.2629593 -0.2279229 ## disp 0.4314492 0.6833358 ## hp -1.0660256 -1.1445907 ## drat 0.4875049 0.4749628 ## wt -2.5456558 -2.7815105 ## qsec 0.9545988 1.0764747 ## vs 0.1815571 0.1724660 ## am 1.1689479 1.1941804 ## gear 0.5097134 0.5117097 ## carb -0.8756238 -0.7712839 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
