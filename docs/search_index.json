[["index.html", "Collected R Code Introduction", " Collected R Code Phil Swatton Introduction This is a collection of R code written for the purposes of practice and demonstration. At the moment this is all in a work in progress stage. The long-term goal is to include lots of statistical models and machine learning algorithms, covering model estimation and data reduction. I may also include some other interesting algorithms down the line. Beyond giving me both a venue for solidifying my understanding and acting as a sort of online advert for my skillset, I also hope that this document will end up being of use to some other people. Im inspired by a similar project by Michael Clark, which you can find at this link. The code here is my own, but the inspiration should be clear. The main difference from Michaels document to mine is that Im placing more emphasis on building more complete functions - i.e.Â they should be workable beyond the example being provided. Theres therefore slightly more of a programming emphasis in this document and its this niche that this document fills. "],["ordinary-least-squares.html", "Ordinary Least Squares", " Ordinary Least Squares ols &lt;- function(data, formula, method=&quot;QR&quot;) { mf &lt;- model.frame(formula, data, na.action=na.exclude) Y &lt;- model.response(mf, type=&quot;double&quot;) X &lt;- model.matrix(formula, mf) if (method == &quot;matrix&quot;) b &lt;- solve(crossprod(X)) %*% crossprod(X, Y) if (method == &quot;QR&quot;) { QR &lt;- qr(X) Q &lt;- qr.Q(QR) R &lt;- qr.R(QR) b &lt;- backsolve(R, crossprod(Q,y)) rownames(b) &lt;- colnames(X) } return(b) } Testing the function: set.seed(42) N &lt;- 1000 X1 &lt;- rnorm(N) X2 &lt;- rnorm(N) y = 1.5 + 2*X1 + 3*X2 + rnorm(N) df &lt;- data.frame( y, X1, X2 ) f &lt;- y ~ X1 + X2 coef(lm(f, df)) ## (Intercept) X1 X2 ## 1.496133 1.978201 2.981712 ols(df, f) ## [,1] ## (Intercept) 1.496133 ## X1 1.978201 ## X2 2.981712 ols(df, f, &quot;matrix&quot;) ## [,1] ## (Intercept) 1.496133 ## X1 1.978201 ## X2 2.981712 "],["logit.html", "Logit", " Logit inv_logit &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) return(p) } logit &lt;- function(data, formula) { mf &lt;- model.frame(formula, data, na.action=na.exclude) Y &lt;- model.response(mf, type=&quot;double&quot;) X &lt;- model.matrix(formula, mf) b &lt;- numeric(ncol(X)) logitll &lt;- function(b) { p &lt;- inv_logit((X %*% b)) lli &lt;- Y*log(p) + (1-Y)*log(1-p) return(-sum(lli)) } est &lt;- optim(b, logitll, method=&quot;BFGS&quot;) b &lt;- est$par names(b) &lt;- colnames(X) return(b) } Testing the function: set.seed(42) N &lt;- 1000 X1 &lt;- rnorm(N) X2 &lt;- rnorm(N) ystar &lt;- 1.5 + 2*X1 + 3*X2 + rnorm(N) y &lt;- rbinom(N, 1, inv_logit(ystar)) df &lt;- data.frame( y, X1, X2 ) f &lt;- y ~ X1 + X2 coef(glm(f,binomial,df)) ## (Intercept) X1 X2 ## 1.413335 1.839204 2.472301 logit(df, f) ## (Intercept) X1 X2 ## 1.413368 1.839245 2.472373 "],["ordered-logit.html", "Ordered Logit", " Ordered Logit inv_logit &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) return(p) } ologit &lt;- function(data, formula, na.action=na.exclude) { mf &lt;- model.frame(formula, data, na.action=na.exclude) Y &lt;- model.response(mf) X &lt;- model.matrix(formula, mf)[,-1] N &lt;- nrow(X) levels &lt;- unique(Y) |&gt; sort() M &lt;- length(levels) ncut &lt;- M-1 c &lt;- 1:ncut/M b &lt;- numeric(ncol(X)) par &lt;- c(c,b) ologitLL &lt;- function(param) { c &lt;- param[1:ncut] b &lt;- param[(ncut+1):length(param)] Xb &lt;- X %*% b lli &lt;- numeric(N) for (m in 1:M) { if (m == 1) { lli[Y==levels[m]] &lt;- log(inv_logit(c[m]-Xb[Y==levels[m]])) } else if (m &lt; M) { lli[Y==levels[m]] &lt;- log(inv_logit(c[m]-Xb[Y==levels[m]]) - inv_logit(c[m-1]-Xb[Y==levels[m]])) } else { lli[Y==levels[m]] &lt;- log(1 - inv_logit(c[m-1]-Xb[Y==levels[m]])) } } return(-sum(lli)) } out &lt;- optim(par, ologitLL, method=&quot;BFGS&quot;) est &lt;- out$par names(est)[1:ncut] &lt;- paste0(&quot;t&quot;, 1:ncut) names(est)[(ncut+1):length(est)] &lt;- colnames(X) return(est) } Testing the function: # Example from https://towardsdatascience.com/implementing-and-interpreting-ordinal-logistic-regression-1ee699274cf5 library(carData) library(MASS) data(WVS) f &lt;- poverty~religion+degree+country+age+gender summary(polr(f, WVS)) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = f, data = WVS) ## ## Coefficients: ## Value Std. Error t value ## religionyes 0.17973 0.077346 2.324 ## degreeyes 0.14092 0.066193 2.129 ## countryNorway -0.32235 0.073766 -4.370 ## countrySweden -0.60330 0.079494 -7.589 ## countryUSA 0.61777 0.070665 8.742 ## age 0.01114 0.001561 7.139 ## gendermale 0.17637 0.052972 3.329 ## ## Intercepts: ## Value Std. Error t value ## Too Little|About Right 0.7298 0.1041 7.0128 ## About Right|Too Much 2.5325 0.1103 22.9496 ## ## Residual Deviance: 10402.59 ## AIC: 10420.59 ologit(WVS, f) ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## t1 t2 religionyes degreeyes countryNorway ## 0.72937251 2.53210358 0.17978268 0.14087551 -0.32235825 ## countrySweden countryUSA age gendermale ## -0.60330084 0.61782030 0.01113128 0.17637937 "],["multinomial-logit.html", "Multinomial Logit", " Multinomial Logit mnlogit &lt;- function(data, formula) { mf &lt;- model.frame(formula, data, na.action=na.exclude) y &lt;- model.response(mf) X &lt;- model.matrix(formula, mf) N &lt;- nrow(X) K &lt;- ncol(X) levels &lt;- unique(y) |&gt; sort() M &lt;- length(levels) Y &lt;- matrix(N*M, nrow=N, ncol=M) for (m in 1:M) { Y[,m] &lt;- ifelse(y==levels[m], 1, 0) } b &lt;- numeric(K*(M-1)) mnlogitLL &lt;- function(param) { b &lt;- matrix(param, nrow=K, ncol=M-1) Xb &lt;- cbind(rep(0, N), X %*% b) lli &lt;- numeric(N) for (m in 1:M) { lli &lt;- lli + Y[,m]*Xb[,m] - Y[,m]*log(rowSums(exp(Xb))) } return(-sum(lli)) } out &lt;- optim(b, mnlogitLL, method=&quot;BFGS&quot;) est &lt;- matrix(out$par, nrow=K, ncol=M-1) rownames(est) &lt;- colnames(X) colnames(est) &lt;- paste0(levels[1], &quot;/&quot;, levels[2:M]) return(est) } Testing the function: library(mclogit) ## Loading required package: Matrix housing &lt;- MASS::housing #has an ordinal outcome but we&#39;ll ignore that for our purposes f &lt;- Sat ~ Infl + Freq + Type t(mblogit(f, data = housing)$coefmat) ## ## Iteration 1 - deviance = 150.7444 - criterion = 0.5116897 ## Iteration 2 - deviance = 150.511 - criterion = 0.001549817 ## Iteration 3 - deviance = 150.5107 - criterion = 1.965799e-06 ## Iteration 4 - deviance = 150.5107 - criterion = 3.804011e-12 ## converged ## Response categories ## Predictors Medium High ## (Intercept) 1.1352190 -0.740346318 ## InflMedium 0.1291512 0.005033149 ## InflHigh -0.4256812 0.282489556 ## Freq -0.0501481 0.026990219 ## TypeApartment 0.8092387 -0.644372087 ## TypeAtrium -0.4053420 0.264568900 ## TypeTerrace -0.3689781 0.189799430 mnlogit(housing, f) ## Low/Medium Low/High ## (Intercept) 1.13582927 -0.740243313 ## InflMedium 0.12920635 0.005028019 ## InflHigh -0.42591879 0.282447109 ## Freq -0.05017388 0.026985033 ## TypeApartment 0.80963960 -0.644298089 ## TypeAtrium -0.40569533 0.264391915 ## TypeTerrace -0.36904115 0.189905490 "],["principal-component-analysis.html", "Principal Component Analysis", " Principal Component Analysis pca &lt;- function(data, scale=T, center=T) { data &lt;- data[complete.cases(data),] if (center) mat &lt;- apply(data, 2, function(x) x - mean(x)) else mat &lt;- data if (scale) for(i in 1:ncol(mat)) mat[,i] &lt;- mat[,i] / sd(data[,i]) decomp &lt;- svd(mat) var &lt;- (decomp$d^2)/(nrow(mat)-1) loadings &lt;- decomp$v colnames(loadings) &lt;- paste0(&quot;PC&quot;, 1:ncol(loadings)) rownames(loadings) &lt;- colnames(mat) return(list(var=var, loadings=loadings)) } Testing the function: # Dataset of English constituency vote shares from UK GE2019 library(parlitools) df &lt;- subset(bes_2019, subset=bes_2019$country==&quot;England&quot;, select=c(&quot;con_19&quot;, &quot;lab_19&quot;, &quot;ld_19&quot;, &quot;brexit_19&quot;, &quot;green_19&quot;, &quot;other_19&quot;)) df &lt;- apply(df, 2, function(x) ifelse(is.na(x), 0, x)) # With centering and rescaling pc &lt;- prcomp(df, scale.=T) pc$sdev^2 ## [1] 2.372841e+00 1.121676e+00 1.022484e+00 8.585543e-01 6.244452e-01 ## [6] 3.622418e-31 pc$rotation ## PC1 PC2 PC3 PC4 PC5 PC6 ## con_19 -0.54332983 0.07627559 -0.05082277 -0.56942560 0.1384571 0.5941178 ## lab_19 0.59823413 -0.07011487 -0.19959561 0.02259843 -0.4083210 0.6558389 ## ld_19 -0.39181511 -0.36512555 0.04767122 0.74424233 0.1153703 0.3790577 ## brexit_19 0.43609277 -0.25251232 0.24198884 -0.11374592 0.8065703 0.1549459 ## green_19 0.04764488 0.75339583 -0.43749954 0.30622933 0.3617630 0.1186169 ## other_19 0.03070171 0.47390033 0.83984620 0.12096531 -0.1391588 0.1874472 pca(df) ## $var ## [1] 2.372841e+00 1.121676e+00 1.022484e+00 8.585543e-01 6.244452e-01 ## [6] 3.628914e-31 ## ## $loadings ## PC1 PC2 PC3 PC4 PC5 PC6 ## con_19 -0.54332983 0.07627559 -0.05082277 -0.56942560 0.1384571 -0.5941178 ## lab_19 0.59823413 -0.07011487 -0.19959561 0.02259843 -0.4083210 -0.6558389 ## ld_19 -0.39181511 -0.36512555 0.04767122 0.74424233 0.1153703 -0.3790577 ## brexit_19 0.43609277 -0.25251232 0.24198884 -0.11374592 0.8065703 -0.1549459 ## green_19 0.04764488 0.75339583 -0.43749954 0.30622933 0.3617630 -0.1186169 ## other_19 0.03070171 0.47390033 0.83984620 0.12096531 -0.1391588 -0.1874472 # Without centering and rescaling pc &lt;- prcomp(df, center=F, scale.=F) pc$sdev^2 ## [1] 3601.582679 528.759516 102.026004 26.105736 14.712369 9.887987 pc$rotation ## PC1 PC2 PC3 PC4 PC5 ## con_19 -0.79601894 0.54348891 0.26225689 -0.02744832 0.02841558 ## lab_19 -0.57199009 -0.81268134 -0.05235555 -0.01872792 -0.07167662 ## ld_19 -0.18775368 0.19101255 -0.96262869 0.03485081 0.01576254 ## brexit_19 -0.03580223 -0.08665429 0.01318959 0.10999656 0.93237245 ## green_19 -0.04785633 -0.01272207 0.01544479 0.07000400 -0.34354616 ## other_19 -0.01891353 0.00349730 0.03759933 0.99029363 -0.08040029 ## PC6 ## con_19 -0.02518186 ## lab_19 -0.06439723 ## ld_19 0.01207155 ## brexit_19 0.33108554 ## green_19 0.93508548 ## other_19 -0.10521720 pca(df, center=F, scale=F) ## $var ## [1] 3601.582679 528.759516 102.026004 26.105736 14.712369 9.887987 ## ## $loadings ## PC1 PC2 PC3 PC4 PC5 ## con_19 -0.79601894 0.54348891 0.26225689 -0.02744832 0.02841558 ## lab_19 -0.57199009 -0.81268134 -0.05235555 -0.01872792 -0.07167662 ## ld_19 -0.18775368 0.19101255 -0.96262869 0.03485081 0.01576254 ## brexit_19 -0.03580223 -0.08665429 0.01318959 0.10999656 0.93237245 ## green_19 -0.04785633 -0.01272207 0.01544479 0.07000400 -0.34354616 ## other_19 -0.01891353 0.00349730 0.03759933 0.99029363 -0.08040029 ## PC6 ## con_19 -0.02518186 ## lab_19 -0.06439723 ## ld_19 0.01207155 ## brexit_19 0.33108554 ## green_19 0.93508548 ## other_19 -0.10521720 "],["lasso-regression---l1-regularization.html", "LASSO Regression - L1 Regularization", " LASSO Regression - L1 Regularization lasso &lt;- function(X, Y, rm_na=T, standardise=F, lambda, tol=1e-6, max_iter=1e+5) { if (!is.matrix(X)) { X &lt;- as.matrix(X) } if (rm_na) { index &lt;- complete.cases(X) &amp; complete.cases(Y) X &lt;- X[index,] Y &lt;- Y[index] } if (standardise) { X &lt;- apply(X, 2, function(x) (x - mean(x))/sd(x)) } # if (intercept) { # X &lt;- cbind(rep(1,nrow(X)), X) # colnames(X)[1] &lt;- &quot;(Intercept)&quot; # } # optim&#39;s methods won&#39;t produce variable selection - see https://stats.stackexchange.com/questions/121209/how-can-i-implement-lasso-in-r-using-optim-function # so estimate via coordinate descent K &lt;- ncol(X) b &lt;- numeric(ncol(X)) # b &lt;- solve(crossprod(X) + lambda*diag(K)) %*% crossprod(X, Y) names(b) &lt;- colnames(X) soft_thresh &lt;- function(b, l) { ifelse(l &lt; abs(b), sign(b)*(abs(b) - l), 0) } current &lt;- 1 for (iter in 1:max_iter) { b_old &lt;- b for (k in 1:K) { r &lt;- Y - X[,-k] %*% b[-k] b[k] &lt;- soft_thresh(crossprod(X[,k],r), length(Y)*lambda)/crossprod(X[,k]) #length(y) gives consistent results w/ glmnet } current &lt;- norm(as.matrix(b-b_old), &quot;F&quot;) if (which.min(c(tol,current))==2) break if (any(is.na(b)) | any(is.nan(b))) break } return(b) } Testing the function: library(glmnet) ## Loaded glmnet 4.1-4 X &lt;- as.matrix(mtcars[, -1]) X_standard &lt;- apply(X, 2, function(x) (x-mean(x)) / sd(x)) Y &lt;- mtcars[[1]] data.frame( glmnet = glmnet(X, Y, alpha=1, lambda=0.5, intercept = F, standardize=F)$beta[,1], lasso = lasso(X, Y, lambda=0.5) ) ## glmnet lasso ## cyl 0.00000000 0.000000000 ## disp -0.01476942 -0.012342881 ## hp -0.00578982 -0.007107273 ## drat 1.24524219 1.226801727 ## wt -0.69362931 -1.034551274 ## qsec 0.94504513 0.993371825 ## vs 0.00000000 0.000000000 ## am 0.00000000 0.000000000 ## gear 1.73327735 1.637404538 ## carb -0.44294482 -0.340473978 data.frame( glmnet = glmnet(X_standard, Y, alpha=1, lambda=0.5, intercept = F, standardize=F)$beta[,1], lasso = lasso(X_standard, Y, lambda=0.5) ) ## glmnet lasso ## cyl -1.51425578 -1.53700527 ## disp 0.00000000 0.00000000 ## hp -0.98389039 -0.96091464 ## drat 0.02978391 0.03332529 ## wt -2.63442748 -2.62683457 ## qsec 0.00000000 0.00000000 ## vs 0.00000000 0.00000000 ## am 0.23140313 0.22850260 ## gear 0.00000000 0.00000000 ## carb -0.15265213 -0.16064929 "],["ridge-regression---l2-regularization.html", "Ridge Regression - L2 Regularization", " Ridge Regression - L2 Regularization ridge &lt;- function(X, Y, rm_na=T, standardise=F, lambda, method=&quot;matrix&quot;) { if (!is.matrix(X)) { X &lt;- as.matrix(X) } if (rm_na) { index &lt;- complete.cases(X) &amp; complete.cases(Y) X &lt;- X[index,] Y &lt;- Y[index] } if (standardise) { X &lt;- apply(X, 2, function(x) (x - mean(x))/sd(x)) } # if (intercept) { # X &lt;- cbind(rep(1,nrow(X)), X) # colnames(X)[1] &lt;- &quot;(Intercept)&quot; # } k &lt;- ncol(X) if (method==&quot;matrix&quot;) { b &lt;- solve(crossprod(X) + lambda*diag(k)) %*% crossprod(X, Y) } else if (method==&quot;MLE&quot;) { start &lt;- numeric(k) ridgeMin &lt;- function(b) { crossprod(Y - X%*%b) + lambda * crossprod(b) } out &lt;- optim(start, ridgeMin, method=&quot;BFGS&quot;) b &lt;- out$par names(b) &lt;- colnames(X) } return(b) } Testing the function: library(glmnet) X &lt;- as.matrix(mtcars[, -1]) X_standard &lt;- apply(X, 2, function(x) (x-mean(x)) / sd(x)) Y &lt;- mtcars[[1]] data.frame( glmnet = glmnet(X, Y, alpha=0, lambda=0.5, intercept = F, standardize=F)$beta[,1], Ridge_Matrix = ridge(X, Y, lambda=0.5), Ridge_MLE = ridge(X, Y, lambda=0.5, method=&quot;MLE&quot;) ) ## glmnet Ridge_Matrix Ridge_MLE ## cyl 0.44401246 0.369554729 0.369555066 ## disp 0.00457116 0.008205706 0.008205705 ## hp -0.01691490 -0.017783212 -0.017783227 ## drat 1.35027698 1.392774438 1.392772924 ## wt -2.82334377 -3.210741706 -3.210737396 ## qsec 1.01961140 1.083489203 1.083488388 ## vs 0.08513161 0.078258643 0.078262480 ## am 2.29971390 2.425849951 2.425855958 ## gear 1.48674773 1.299391499 1.299392402 ## carb -0.59355808 -0.469904311 -0.469904608 data.frame( glmnet = glmnet(X_standard, Y, alpha=0, lambda=0.5, intercept = F, standardize=F)$beta[,1], Ridge_Matrix = ridge(X_standard, Y, lambda=0.5), Ridge_MLE = ridge(X_standard, Y, lambda=0.5, method=&quot;MLE&quot;) ) ## glmnet Ridge_Matrix Ridge_MLE ## cyl -0.2664494 -0.2279229 -0.2278197 ## disp 0.4681466 0.6833358 0.6833124 ## hp -1.0616275 -1.1445907 -1.1446305 ## drat 0.4776239 0.4749628 0.4749703 ## wt -2.5629699 -2.7815105 -2.7814765 ## qsec 0.9656145 1.0764747 1.0764682 ## vs 0.1892789 0.1724660 0.1725038 ## am 1.1745870 1.1941804 1.1941926 ## gear 0.5221163 0.5117097 0.5117464 ## carb -0.8771296 -0.7712839 -0.7713067 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
