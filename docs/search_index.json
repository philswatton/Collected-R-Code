[["index.html", "Collected R Code Introduction", " Collected R Code Phil Swatton Introduction This is a collection of R code written for the purposes of practice and demonstration. At the moment this is all in a work in progress stage. The long-term goal is to include lots of statistical models and machine learning algorithms, covering model estimation and data reduction. I may also include some other interesting algorithms down the line. Beyond giving me both a venue for solidifying my understanding and acting as a sort of online advert for my skillset, I also hope that this document will end up being of use to some other people. I’m inspired by a similar project by Michael Clark, which you can find at this link. The code here is my own, but the inspiration should be clear. The main difference from Michael’s document to mine is that I’m placing more emphasis on building more ‘complete’ functions - i.e. they should be workable beyond the example being provided. There’s therefore slightly more of a programming emphasis in this document and it’s this niche that this document fills. "],["ordinary-least-squares.html", "Ordinary Least Squares Solutions Implementation", " Ordinary Least Squares we want to solve something of the form \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\] Solutions OLS is typically learned as \\[ (\\boldsymbol{X}&#39;\\boldsymbol{X})^{-1}\\boldsymbol{X}&#39;\\boldsymbol{Y} \\] - but in practice, this is computationally inefficient - floating point algebra, matrix inversion, etc etc - instead, we use QR decomposition QR Decomposition \\[ \\boldsymbol{X} = \\boldsymbol{QR} \\] - since R is an upper triangular matrix, we can set \\[ \\boldsymbol{Y} = \\boldsymbol{QR}\\boldsymbol{\\beta} \\] - meaning that: \\[ \\boldsymbol{Q}^{-1}\\boldsymbol{Y} = \\boldsymbol{R}\\boldsymbol{\\beta} \\] - by definition of the QR decomposition: \\[ \\boldsymbol{Q}^{-1}\\boldsymbol{Y} = \\boldsymbol{Q}&#39;\\boldsymbol{Y}\\] - we can therefore exploit the triangular structur of \\(\\boldsymbol{R}\\) to backsolve the equation: \\[ \\boldsymbol{Q}&#39;\\boldsymbol{Y} = \\boldsymbol{R}\\boldsymbol{\\beta} \\] LU Decomposition \\[ \\left(\\boldsymbol{X}&#39;\\boldsymbol{X}\\right)\\boldsymbol{b} = \\boldsymbol{X}&#39;\\boldsymbol{y} \\] Implementation ols &lt;- function(data, formula, method=&quot;QR&quot;) { mf &lt;- model.frame(formula, data, na.action=na.exclude) Y &lt;- model.response(mf, type=&quot;double&quot;) X &lt;- model.matrix(formula, mf) if (method == &quot;matrix&quot;) b &lt;- solve(crossprod(X)) %*% crossprod(X, Y) if (method == &quot;QR&quot;) { QR &lt;- qr(X) Q &lt;- qr.Q(QR) R &lt;- qr.R(QR) b &lt;- backsolve(R, crossprod(Q,y)) rownames(b) &lt;- colnames(X) } return(b) } Testing the function: set.seed(42) N &lt;- 1000 X1 &lt;- rnorm(N) X2 &lt;- rnorm(N) y = 1.5 + 2*X1 + 3*X2 + rnorm(N) df &lt;- data.frame( y, X1, X2 ) f &lt;- y ~ X1 + X2 coef(lm(f, df)) ## (Intercept) X1 X2 ## 1.496133 1.978201 2.981712 ols(df, f) ## [,1] ## (Intercept) 1.496133 ## X1 1.978201 ## X2 2.981712 ols(df, f, &quot;matrix&quot;) ## [,1] ## (Intercept) 1.496133 ## X1 1.978201 ## X2 2.981712 "],["logit.html", "Logit", " Logit inv_logit &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) return(p) } logit &lt;- function(data, formula) { mf &lt;- model.frame(formula, data, na.action=na.exclude) Y &lt;- model.response(mf, type=&quot;double&quot;) X &lt;- model.matrix(formula, mf) b &lt;- numeric(ncol(X)) logitll &lt;- function(b) { p &lt;- inv_logit((X %*% b)) lli &lt;- Y*log(p) + (1-Y)*log(1-p) return(-sum(lli)) } est &lt;- optim(b, logitll, method=&quot;BFGS&quot;) b &lt;- est$par names(b) &lt;- colnames(X) return(b) } Testing the function: set.seed(42) N &lt;- 1000 X1 &lt;- rnorm(N) X2 &lt;- rnorm(N) ystar &lt;- 1.5 + 2*X1 + 3*X2 + rnorm(N) y &lt;- rbinom(N, 1, inv_logit(ystar)) df &lt;- data.frame( y, X1, X2 ) f &lt;- y ~ X1 + X2 coef(glm(f,binomial,df)) ## (Intercept) X1 X2 ## 1.413335 1.839204 2.472301 logit(df, f) ## (Intercept) X1 X2 ## 1.413368 1.839245 2.472373 "],["ordered-logit.html", "Ordered Logit", " Ordered Logit inv_logit &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) return(p) } ologit &lt;- function(data, formula, na.action=na.exclude) { mf &lt;- model.frame(formula, data, na.action=na.exclude) Y &lt;- model.response(mf) X &lt;- model.matrix(formula, mf)[,-1] N &lt;- nrow(X) levels &lt;- unique(Y) |&gt; sort() M &lt;- length(levels) ncut &lt;- M-1 c &lt;- 1:ncut/M b &lt;- numeric(ncol(X)) par &lt;- c(c,b) ologitLL &lt;- function(param) { c &lt;- param[1:ncut] b &lt;- param[(ncut+1):length(param)] Xb &lt;- X %*% b lli &lt;- numeric(N) for (m in 1:M) { if (m == 1) { lli[Y==levels[m]] &lt;- log(inv_logit(c[m]-Xb[Y==levels[m]])) } else if (m &lt; M) { lli[Y==levels[m]] &lt;- log(inv_logit(c[m]-Xb[Y==levels[m]]) - inv_logit(c[m-1]-Xb[Y==levels[m]])) } else { lli[Y==levels[m]] &lt;- log(1 - inv_logit(c[m-1]-Xb[Y==levels[m]])) } } return(-sum(lli)) } out &lt;- optim(par, ologitLL, method=&quot;BFGS&quot;) est &lt;- out$par names(est)[1:ncut] &lt;- paste0(&quot;t&quot;, 1:ncut) names(est)[(ncut+1):length(est)] &lt;- colnames(X) return(est) } Testing the function: # Example from https://towardsdatascience.com/implementing-and-interpreting-ordinal-logistic-regression-1ee699274cf5 library(carData) library(MASS) data(WVS) f &lt;- poverty~religion+degree+country+age+gender summary(polr(f, WVS)) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = f, data = WVS) ## ## Coefficients: ## Value Std. Error t value ## religionyes 0.17973 0.077346 2.324 ## degreeyes 0.14092 0.066193 2.129 ## countryNorway -0.32235 0.073766 -4.370 ## countrySweden -0.60330 0.079494 -7.589 ## countryUSA 0.61777 0.070665 8.742 ## age 0.01114 0.001561 7.139 ## gendermale 0.17637 0.052972 3.329 ## ## Intercepts: ## Value Std. Error t value ## Too Little|About Right 0.7298 0.1041 7.0128 ## About Right|Too Much 2.5325 0.1103 22.9496 ## ## Residual Deviance: 10402.59 ## AIC: 10420.59 ologit(WVS, f) ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## Warning in log(inv_logit(c[m] - Xb[Y == levels[m]]) - inv_logit(c[m - 1] - : ## NaNs produced ## t1 t2 religionyes degreeyes countryNorway ## 0.72937251 2.53210358 0.17978268 0.14087551 -0.32235825 ## countrySweden countryUSA age gendermale ## -0.60330084 0.61782030 0.01113128 0.17637937 "],["multinomial-logit.html", "Multinomial Logit", " Multinomial Logit mnlogit &lt;- function(data, formula) { mf &lt;- model.frame(formula, data, na.action=na.exclude) y &lt;- model.response(mf) X &lt;- model.matrix(formula, mf) N &lt;- nrow(X) K &lt;- ncol(X) levels &lt;- unique(y) |&gt; sort() M &lt;- length(levels) Y &lt;- matrix(N*M, nrow=N, ncol=M) for (m in 1:M) { Y[,m] &lt;- ifelse(y==levels[m], 1, 0) } b &lt;- numeric(K*(M-1)) mnlogitLL &lt;- function(param) { b &lt;- matrix(param, nrow=K, ncol=M-1) Xb &lt;- cbind(rep(0, N), X %*% b) lli &lt;- numeric(N) for (m in 1:M) { lli &lt;- lli + Y[,m]*Xb[,m] - Y[,m]*log(rowSums(exp(Xb))) } return(-sum(lli)) } out &lt;- optim(b, mnlogitLL, method=&quot;BFGS&quot;) est &lt;- matrix(out$par, nrow=K, ncol=M-1) rownames(est) &lt;- colnames(X) colnames(est) &lt;- paste0(levels[1], &quot;/&quot;, levels[2:M]) return(est) } Testing the function: library(mclogit) ## Loading required package: Matrix housing &lt;- MASS::housing #has an ordinal outcome but we&#39;ll ignore that for our purposes f &lt;- Sat ~ Infl + Freq + Type t(mblogit(f, data = housing)$coefmat) ## ## Iteration 1 - deviance = 150.7444 - criterion = 0.5116897 ## Iteration 2 - deviance = 150.511 - criterion = 0.001549817 ## Iteration 3 - deviance = 150.5107 - criterion = 1.965799e-06 ## Iteration 4 - deviance = 150.5107 - criterion = 3.804011e-12 ## converged ## Response categories ## Predictors Medium High ## (Intercept) 1.1352190 -0.740346318 ## InflMedium 0.1291512 0.005033149 ## InflHigh -0.4256812 0.282489556 ## Freq -0.0501481 0.026990219 ## TypeApartment 0.8092387 -0.644372087 ## TypeAtrium -0.4053420 0.264568900 ## TypeTerrace -0.3689781 0.189799430 mnlogit(housing, f) ## Low/Medium Low/High ## (Intercept) 1.13582927 -0.740243313 ## InflMedium 0.12920635 0.005028019 ## InflHigh -0.42591879 0.282447109 ## Freq -0.05017388 0.026985033 ## TypeApartment 0.80963960 -0.644298089 ## TypeAtrium -0.40569533 0.264391915 ## TypeTerrace -0.36904115 0.189905490 "],["principal-component-analysis.html", "Principal Component Analysis", " Principal Component Analysis pca &lt;- function(data, scale=T, center=T) { data &lt;- data[complete.cases(data),] if (center) mat &lt;- apply(data, 2, function(x) x - mean(x)) else mat &lt;- data if (scale) for(i in 1:ncol(mat)) mat[,i] &lt;- mat[,i] / sd(data[,i]) decomp &lt;- svd(mat) var &lt;- (decomp$d^2)/(nrow(mat)-1) loadings &lt;- decomp$v colnames(loadings) &lt;- paste0(&quot;PC&quot;, 1:ncol(loadings)) rownames(loadings) &lt;- colnames(mat) return(list(var=var, loadings=loadings)) } Testing the function: # Dataset of English constituency vote shares from UK GE2019 library(parlitools) df &lt;- subset(bes_2019, subset=bes_2019$country==&quot;England&quot;, select=c(&quot;con_19&quot;, &quot;lab_19&quot;, &quot;ld_19&quot;, &quot;brexit_19&quot;, &quot;green_19&quot;, &quot;other_19&quot;)) df &lt;- apply(df, 2, function(x) ifelse(is.na(x), 0, x)) # With centering and rescaling pc &lt;- prcomp(df, scale.=T) pc$sdev^2 ## [1] 2.372841e+00 1.121676e+00 1.022484e+00 8.585543e-01 6.244452e-01 ## [6] 3.622418e-31 pc$rotation ## PC1 PC2 PC3 PC4 PC5 PC6 ## con_19 -0.54332983 0.07627559 -0.05082277 -0.56942560 0.1384571 0.5941178 ## lab_19 0.59823413 -0.07011487 -0.19959561 0.02259843 -0.4083210 0.6558389 ## ld_19 -0.39181511 -0.36512555 0.04767122 0.74424233 0.1153703 0.3790577 ## brexit_19 0.43609277 -0.25251232 0.24198884 -0.11374592 0.8065703 0.1549459 ## green_19 0.04764488 0.75339583 -0.43749954 0.30622933 0.3617630 0.1186169 ## other_19 0.03070171 0.47390033 0.83984620 0.12096531 -0.1391588 0.1874472 pca(df) ## $var ## [1] 2.372841e+00 1.121676e+00 1.022484e+00 8.585543e-01 6.244452e-01 ## [6] 3.628914e-31 ## ## $loadings ## PC1 PC2 PC3 PC4 PC5 PC6 ## con_19 -0.54332983 0.07627559 -0.05082277 -0.56942560 0.1384571 -0.5941178 ## lab_19 0.59823413 -0.07011487 -0.19959561 0.02259843 -0.4083210 -0.6558389 ## ld_19 -0.39181511 -0.36512555 0.04767122 0.74424233 0.1153703 -0.3790577 ## brexit_19 0.43609277 -0.25251232 0.24198884 -0.11374592 0.8065703 -0.1549459 ## green_19 0.04764488 0.75339583 -0.43749954 0.30622933 0.3617630 -0.1186169 ## other_19 0.03070171 0.47390033 0.83984620 0.12096531 -0.1391588 -0.1874472 # Without centering and rescaling pc &lt;- prcomp(df, center=F, scale.=F) pc$sdev^2 ## [1] 3601.582679 528.759516 102.026004 26.105736 14.712369 9.887987 pc$rotation ## PC1 PC2 PC3 PC4 PC5 ## con_19 -0.79601894 0.54348891 0.26225689 -0.02744832 0.02841558 ## lab_19 -0.57199009 -0.81268134 -0.05235555 -0.01872792 -0.07167662 ## ld_19 -0.18775368 0.19101255 -0.96262869 0.03485081 0.01576254 ## brexit_19 -0.03580223 -0.08665429 0.01318959 0.10999656 0.93237245 ## green_19 -0.04785633 -0.01272207 0.01544479 0.07000400 -0.34354616 ## other_19 -0.01891353 0.00349730 0.03759933 0.99029363 -0.08040029 ## PC6 ## con_19 -0.02518186 ## lab_19 -0.06439723 ## ld_19 0.01207155 ## brexit_19 0.33108554 ## green_19 0.93508548 ## other_19 -0.10521720 pca(df, center=F, scale=F) ## $var ## [1] 3601.582679 528.759516 102.026004 26.105736 14.712369 9.887987 ## ## $loadings ## PC1 PC2 PC3 PC4 PC5 ## con_19 -0.79601894 0.54348891 0.26225689 -0.02744832 0.02841558 ## lab_19 -0.57199009 -0.81268134 -0.05235555 -0.01872792 -0.07167662 ## ld_19 -0.18775368 0.19101255 -0.96262869 0.03485081 0.01576254 ## brexit_19 -0.03580223 -0.08665429 0.01318959 0.10999656 0.93237245 ## green_19 -0.04785633 -0.01272207 0.01544479 0.07000400 -0.34354616 ## other_19 -0.01891353 0.00349730 0.03759933 0.99029363 -0.08040029 ## PC6 ## con_19 -0.02518186 ## lab_19 -0.06439723 ## ld_19 0.01207155 ## brexit_19 0.33108554 ## green_19 0.93508548 ## other_19 -0.10521720 "],["gaussian-mixture-models.html", "Gaussian Mixture Models Univariate Gaussian Mixture Models", " Gaussian Mixture Models Univariate Gaussian Mixture Models Implementation ugmm &lt;- function(X, K, tol=0.0001, maxit=1000) { # Basics of X N &lt;- length(X) # Initialise means mu &lt;- sample(X, K) # Initialise variances theta &lt;- rep(var(X), K) # Initialise mixing proportions prop &lt;- rep(1/K, K) # Initialise individual label probabilities and weighted densities g &lt;- matrix(0, nrow=N, ncol=K) # Initialise the difference diff &lt;- Inf old &lt;- 0 # Initialise iterations it &lt;- 0 # EM Algo while (it &lt; maxit) { # Old values oldmu &lt;- mu oldtheta &lt;- theta oldprop &lt;- prop # E-step for (k in 1:K) { g[,k] &lt;- prop[k] * dnorm(X, mu[k], sqrt(theta[k])) } g &lt;- g / rowSums(g) # M-step Nk &lt;- colSums(g) for (k in 1:K) { # Calculate mu_k mu[k] &lt;- sum(g[,k] * X) / Nk[k] # Calculate theta^2_k theta[k] &lt;- sum(g[,k]*(X - mu[k])^2) / Nk[k] # Update mixing proportions prop[k] &lt;- Nk[k]/N } # Calculate log-likelihood lli &lt;- numeric(N) for (k in 1:K) { lli &lt;- lli + g[,k] * (log(prop[k]) + log(dnorm(X, mu[k], sqrt(theta[k])))) } ll &lt;- sum(lli) # Recalulate difference diff &lt;- abs(ll - old) # Update iterations it &lt;- it+1 # Check if (diff &lt; tol) break # Assume the loop hasn&#39;t ended, update the old ll old &lt;- ll } return(list(it=it, mu=mu, theta=sqrt(theta), prop=prop)) } Testing the function: # Simulate draws from 3 distributions set.seed(1234) x1 &lt;- rnorm(1000, 0, 1) x2 &lt;- rnorm(1500, -4, 2) x3 &lt;- rnorm(500, 2, 0.5) # Combine into single vector x &lt;- c(x1, x2, x3) # Density Plot plot(density(x)) # Estimate clusters ugmm(x, 3) ## $it ## [1] 231 ## ## $mu ## [1] -3.9174651 1.9712504 -0.1162445 ## ## $theta ## [1] 2.0231925 0.5045694 0.8715375 ## ## $prop ## [1] 0.5054033 0.1863845 0.3082122 "],["lasso-regression---l1-regularization.html", "LASSO Regression - L1 Regularization", " LASSO Regression - L1 Regularization lasso &lt;- function(X, Y, rm_na=T, standardise=F, lambda, tol=1e-6, max_iter=1e+5) { if (!is.matrix(X)) { X &lt;- as.matrix(X) } if (rm_na) { index &lt;- complete.cases(X) &amp; complete.cases(Y) X &lt;- X[index,] Y &lt;- Y[index] } if (standardise) { X &lt;- apply(X, 2, function(x) (x - mean(x))/sd(x)) } # if (intercept) { # X &lt;- cbind(rep(1,nrow(X)), X) # colnames(X)[1] &lt;- &quot;(Intercept)&quot; # } # optim&#39;s methods won&#39;t produce variable selection - see https://stats.stackexchange.com/questions/121209/how-can-i-implement-lasso-in-r-using-optim-function # so estimate via coordinate descent K &lt;- ncol(X) b &lt;- numeric(ncol(X)) # b &lt;- solve(crossprod(X) + lambda*diag(K)) %*% crossprod(X, Y) names(b) &lt;- colnames(X) soft_thresh &lt;- function(b, l) { ifelse(l &lt; abs(b), sign(b)*(abs(b) - l), 0) } current &lt;- 1 for (iter in 1:max_iter) { b_old &lt;- b for (k in 1:K) { r &lt;- Y - X[,-k] %*% b[-k] b[k] &lt;- soft_thresh(crossprod(X[,k],r), length(Y)*lambda)/crossprod(X[,k]) #length(y) gives consistent results w/ glmnet } current &lt;- norm(as.matrix(b-b_old), &quot;F&quot;) if (which.min(c(tol,current))==2) break if (any(is.na(b)) | any(is.nan(b))) break } return(b) } Testing the function: library(glmnet) ## Loaded glmnet 4.1-4 X &lt;- as.matrix(mtcars[, -1]) X_standard &lt;- apply(X, 2, function(x) (x-mean(x)) / sd(x)) Y &lt;- mtcars[[1]] data.frame( glmnet = glmnet(X, Y, alpha=1, lambda=0.5, intercept = F, standardize=F)$beta[,1], lasso = lasso(X, Y, lambda=0.5) ) ## glmnet lasso ## cyl 0.00000000 0.000000000 ## disp -0.01476942 -0.012342881 ## hp -0.00578982 -0.007107273 ## drat 1.24524219 1.226801727 ## wt -0.69362931 -1.034551274 ## qsec 0.94504513 0.993371825 ## vs 0.00000000 0.000000000 ## am 0.00000000 0.000000000 ## gear 1.73327735 1.637404538 ## carb -0.44294482 -0.340473978 data.frame( glmnet = glmnet(X_standard, Y, alpha=1, lambda=0.5, intercept = F, standardize=F)$beta[,1], lasso = lasso(X_standard, Y, lambda=0.5) ) ## glmnet lasso ## cyl -1.51425578 -1.53700527 ## disp 0.00000000 0.00000000 ## hp -0.98389039 -0.96091464 ## drat 0.02978391 0.03332529 ## wt -2.63442748 -2.62683457 ## qsec 0.00000000 0.00000000 ## vs 0.00000000 0.00000000 ## am 0.23140313 0.22850260 ## gear 0.00000000 0.00000000 ## carb -0.15265213 -0.16064929 "],["ridge-regression---l2-regularization.html", "Ridge Regression - L2 Regularization", " Ridge Regression - L2 Regularization ridge &lt;- function(X, Y, rm_na=T, standardise=F, lambda, method=&quot;matrix&quot;) { if (!is.matrix(X)) { X &lt;- as.matrix(X) } if (rm_na) { index &lt;- complete.cases(X) &amp; complete.cases(Y) X &lt;- X[index,] Y &lt;- Y[index] } if (standardise) { X &lt;- apply(X, 2, function(x) (x - mean(x))/sd(x)) } # if (intercept) { # X &lt;- cbind(rep(1,nrow(X)), X) # colnames(X)[1] &lt;- &quot;(Intercept)&quot; # } k &lt;- ncol(X) if (method==&quot;matrix&quot;) { b &lt;- solve(crossprod(X) + lambda*diag(k)) %*% crossprod(X, Y) } else if (method==&quot;MLE&quot;) { start &lt;- numeric(k) ridgeMin &lt;- function(b) { crossprod(Y - X%*%b) + lambda * crossprod(b) } out &lt;- optim(start, ridgeMin, method=&quot;BFGS&quot;) b &lt;- out$par names(b) &lt;- colnames(X) } return(b) } Testing the function: library(glmnet) X &lt;- as.matrix(mtcars[, -1]) X_standard &lt;- apply(X, 2, function(x) (x-mean(x)) / sd(x)) Y &lt;- mtcars[[1]] data.frame( glmnet = glmnet(X, Y, alpha=0, lambda=0.5, intercept = F, standardize=F)$beta[,1], Ridge_Matrix = ridge(X, Y, lambda=0.5), Ridge_MLE = ridge(X, Y, lambda=0.5, method=&quot;MLE&quot;) ) ## glmnet Ridge_Matrix Ridge_MLE ## cyl 0.44401246 0.369554729 0.369555066 ## disp 0.00457116 0.008205706 0.008205705 ## hp -0.01691490 -0.017783212 -0.017783227 ## drat 1.35027698 1.392774438 1.392772924 ## wt -2.82334377 -3.210741706 -3.210737396 ## qsec 1.01961140 1.083489203 1.083488388 ## vs 0.08513161 0.078258643 0.078262480 ## am 2.29971390 2.425849951 2.425855958 ## gear 1.48674773 1.299391499 1.299392402 ## carb -0.59355808 -0.469904311 -0.469904608 data.frame( glmnet = glmnet(X_standard, Y, alpha=0, lambda=0.5, intercept = F, standardize=F)$beta[,1], Ridge_Matrix = ridge(X_standard, Y, lambda=0.5), Ridge_MLE = ridge(X_standard, Y, lambda=0.5, method=&quot;MLE&quot;) ) ## glmnet Ridge_Matrix Ridge_MLE ## cyl -0.2664494 -0.2279229 -0.2278197 ## disp 0.4681466 0.6833358 0.6833124 ## hp -1.0616275 -1.1445907 -1.1446305 ## drat 0.4776239 0.4749628 0.4749703 ## wt -2.5629699 -2.7815105 -2.7814765 ## qsec 0.9656145 1.0764747 1.0764682 ## vs 0.1892789 0.1724660 0.1725038 ## am 1.1745870 1.1941804 1.1941926 ## gear 0.5221163 0.5117097 0.5117464 ## carb -0.8771296 -0.7712839 -0.7713067 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
